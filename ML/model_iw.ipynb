{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix, RocCurveDisplay, confusion_matrix, classification_report, plot_roc_curve, roc_auc_score, make_scorer, fbeta_score,accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from time import time\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import scikitplot as skplt\n",
    "import plotly.express as px\n",
    "import folium\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier , plot_tree\n",
    "# import eli5 \n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import warnings \n",
    "\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"hotel_bookings.csv\")\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\n",
    "pd.options.display.max_rows = 200000\n",
    "data_df = data_df.drop(columns=[\"reservation_status_date\", \"hotel\", \"reservation_status\",\n",
    "    \"arrival_date_week_number\", \"reserved_room_type\", \"company\"])\n",
    "\n",
    "data_df[\"total_guests\"] = data_df[\"adults\"] + data_df[\"children\"] + data_df[\"babies\"]\n",
    "data_df[\"price_per_guest\"] = data_df[\"adr\"]/data_df[\"total_guests\"]\n",
    "\n",
    "filter = (data_df.children == 0) & (data_df.adults == 0) & (data_df.babies == 0)\n",
    "data_df[filter]\n",
    "data_df = data_df[~filter]\n",
    "data_df=data_df.replace(\" \",\"_\")\n",
    "data_df = data_df.reindex(data_df.columns.tolist(), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_replacements = {\"children\": 0.0, \"agent\": 0, \"country\": \"unknown\", \"total_guests\":0, \"price_per_guest\":0}\n",
    "data_df = data_df.fillna(nan_replacements)\n",
    "# data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df[\"price_per_guest\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119210, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# czy usunąć też: data_df[\"price_per_guest\"] ==0???\n",
    "\n",
    "data_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Drop rows with NaN\n",
    "data_df.dropna(inplace=True)\n",
    "\n",
    "data_df.drop(14969)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja zwracająca cechy numeryczne\n",
    "def get_quant_features(dataframe):\n",
    "    feats = dataframe.select_dtypes([np.number, np.bool]).columns\n",
    "    return [x for x in feats]\n",
    "\n",
    "# funkcja rysująca histogramy dla numerycznych cechy\n",
    "def draw_histograms(dataframe, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(16,10))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
    "        dataframe[var_name].hist(bins=20,ax=ax)\n",
    "        ax.set_title(var_name)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_quant_features(data_df)\n",
    "print(len(feats))\n",
    "draw_histograms(data_df, feats, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"]=(16,8)\n",
    "sns.heatmap(data_df.corr(method=\"pearson\",), vmax=1., vmin=-1., annot=True, linewidths=.8, cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x=\"arrival_date_month\", hue=\"is_canceled\", data=data_df)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x=\"arrival_date_month\", y=\"is_canceled\", hue=\"arrival_date_year\", data=data_df)\n",
    "plt.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzamy, czy są jakieś wartości odstające\n",
    "# pd.diff() sprawdza różnice co do poprzedniego elementu\n",
    "plt.plot(data_df.index, data_df[\"is_canceled\"].diff(), \"bo\", alpha=0.3, ms=10, lw=\"3\")\n",
    "\n",
    "plt.hlines(xmin=data_df.index.min(), xmax=data_df.index.max(), y=100, color=\"r\", linestyle=\"-.\")\n",
    "plt.hlines(xmin=data_df.index.min(), xmax=data_df.index.max(), y=-100, color=\"r\", linestyle=\"-.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "# heatmap dla średniej PM2.5 dla godzin i miesięcy\n",
    "df_train = data_df.groupby([\"arrival_date_month\", \"arrival_date_day_of_month\"])[\"is_canceled\"].mean().reset_index()\n",
    "df_train = df_train.pivot(\"arrival_date_month\", \"arrival_date_day_of_month\", \"is_canceled\")\n",
    "ax = sns.heatmap(df_train, cmap=\"BuPu\", square=True)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Month\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.get_dummies(data_df)\n",
    "# data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_df.drop(columns=[\"is_canceled\",\"total_guests\"], axis=1).columns\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_df[features]\n",
    "y = data_df[\"is_canceled\"]\n",
    "# y = np.array(data_df[\"is_canceled\"])\n",
    "\n",
    "sep_cat= []\n",
    "sep_num = []\n",
    "for n in features:\n",
    "    if data_df[n].dtype == object:\n",
    "        sep_cat.append(n)\n",
    "    else:\n",
    "        sep_num.append(n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_rest, y_train, y_rest = train_test_split(x, y, test_size=0.40, random_state = 42)\n",
    "# x_val, x_test, y_val, y_test = train_test_split(x, y, test_size=0.50, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x, y=None, **fit_params):\n",
    "        return x.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess numerical feats:\n",
    "# for most num cols, except the dates, 0 is the most logical choice as fill value and here no dates are missing.\n",
    "sep_num_transformer = SimpleImputer(strategy=\"constant\")\n",
    "\n",
    "# Preprocessing for categorical features:\n",
    "sep_cat_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    (\"to_dense\", DenseTransformer())])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical features:\n",
    "datacleaner = ColumnTransformer(transformers=[(\"num\", sep_num_transformer, sep_num),\n",
    "                                              (\"cat\", sep_cat_transformer, sep_cat)],remainder = \"passthrough\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy_Model:  f-beta: 0.3706, accuracy score: 0.531, time: 3.3076\n",
      "DecisionTree_Model  f-beta: 0.8087, accuracy score: 0.859, time: 25.3144\n",
      "RandomForest_Model  f-beta: 0.8732, accuracy score: 0.8915, time: 284.7856\n",
      "LogisticRegression_Model  f-beta: 0.773, accuracy score: 0.8134, time: 14.242\n",
      "XGBBoost_Model  f-beta: 0.8451, accuracy score: 0.877, time: 98.223\n",
      "Gaussian_Model  f-beta: 0.4894, accuracy score: 0.5321, time: 6.2447\n",
      "KNN_Model  f-beta: 0.7002, accuracy score: 0.7744, time: 104.7729\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# define models to test:\n",
    "models = [(\"Dummy_Model:\",DummyClassifier(strategy=\"stratified\")),\n",
    "            (\"DecisionTree_Model\", DecisionTreeClassifier(random_state=42)),\n",
    "            (\"RandomForest_Model\", RandomForestClassifier(random_state=42)),\n",
    "            (\"LogisticRegression_Model\", LogisticRegression(random_state=42, solver=\"liblinear\")),\n",
    "            (\"XGBBoost_Model\", XGBClassifier(random_state=42,use_label_encoder=False, eval_metric=\"mlogloss\", n_jobs=-1)),\n",
    "            (\"Gaussian_Model\", GaussianNB()),\n",
    "            (\"KNN_Model\", KNeighborsClassifier())]\n",
    "\n",
    "\n",
    "# split data into \"kfolds\" parts for cross validation,\n",
    "# use shuffle to ensure random distribution of data:\n",
    "kfolds = 4 # 4 = 75% train, 25% validation\n",
    "split = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n",
    "fbeta_scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "# Preprocessing, fitting, making predictions and scoring for every model:\n",
    "for name, model in models:\n",
    "    # pack preprocessing of data and the model in a pipeline:\n",
    "    model_steps = Pipeline(steps=[(\"datacleaner\", datacleaner), (\"model\", model)])\n",
    "    start = time()\n",
    "    cv_results = cross_val_score(model_steps, x, y, cv=split, scoring=\"accuracy\")\n",
    "    fbeta_res = cross_val_score(model_steps, x, y, cv=split, scoring=fbeta_scorer)\n",
    "    mean_score = round(np.mean(cv_results), 4)\n",
    "    fbeta_score = round(np.mean(fbeta_res), 4)\n",
    "    end = time()\n",
    "    cross_val_time = round(end-start,4)\n",
    "    print(f\"{name}  f-beta: {fbeta_score}, accuracy score: {mean_score}, time: {cross_val_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(solver=\"liblinear\")\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "# y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Raport klasyfikacyjny:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "# print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "# print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "# print(\"Fbeta:\", metrics.fbeta_score(y_test, y_pred, beta=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "# group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "# group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()] #flatten() \"splaszcza\" matryce\n",
    "\n",
    "# labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
    "# labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "# sns.set(rc={\"figure.figsize\":(8,6)})\n",
    "# ax = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\")\n",
    "# ax.set(xlabel=\"Predicted\", ylabel=\"Actual\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc_curve(model, x_train, y_train)\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skplt.estimators.plot_learning_curve(model, x, y)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(x, y)\n",
    "# importance = np.abs(ridge.coef_)\n",
    "# feature_names = np.array(x.columns)\n",
    "# feature_names.shape\n",
    "\n",
    "# plt.bar(height=importance, x=feature_names)\n",
    "# plt.title(\"Feature importances via coefficients\")\n",
    "# plt.show()\n",
    "\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# from time import time\n",
    "\n",
    "# threshold = np.sort(importance)[-120] + 0.01\n",
    "\n",
    "# tic = time()\n",
    "# sfm = SelectFromModel(ridge, threshold=threshold).fit(x, y)\n",
    "# toc = time()\n",
    "# print(f\"Features selected by SelectFromModel: {feature_names[sfm.get_support()]}\")\n",
    "# print(f\"Done in {toc - tic:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_n = data_df[feature_names[sfm.get_support()]]\n",
    "# y_n = np.array(data_df[\"is_canceled\"])\n",
    "\n",
    "# x_train_n, x_rest_n, y_train_n, y_rest_n = train_test_split(x_n, y_n, test_size=0.40, random_state = 2022)\n",
    "# x_val_n, x_test_n, y_val_n, y_test_n = train_test_split(x_n, y_n, test_size=0.50, random_state = 2022)\n",
    "\n",
    "# model2 = LogisticRegression(solver=\"liblinear\")\n",
    "# model2.fit(x_train_n, y_train_n)\n",
    "\n",
    "# y_pred_n = model2.predict(x_test_n)\n",
    "\n",
    "# print(\"Raport klasyfikacyjny:\\n\", classification_report(y_test_n, y_pred_n))\n",
    "# print(\"Accuracy:\", metrics.accuracy_score(y_test_n, y_pred_n))\n",
    "# print(\"Precision:\", metrics.precision_score(y_test_n, y_pred_n))\n",
    "# print(\"Recall:\", metrics.recall_score(y_test_n, y_pred_n))\n",
    "# print(\"Fbeta:\", metrics.fbeta_score(y_test_n, y_pred_n, beta=0.5))\n",
    "\n",
    "# plot_roc_curve(model2, x_train_n, y_train_n)\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# tic_fwd = time()\n",
    "# sfs_forward = SequentialFeatureSelector(\n",
    "#     ridge, n_features_to_select=2, direction=\"forward\"\n",
    "# ).fit(x, y)\n",
    "# toc_fwd = time()\n",
    "\n",
    "# tic_bwd = time()\n",
    "# sfs_backward = SequentialFeatureSelector(\n",
    "#     ridge, n_features_to_select=2, direction=\"backward\"\n",
    "# ).fit(x, y)\n",
    "# toc_bwd = time()\n",
    "\n",
    "# print(\n",
    "#     \"Features selected by forward sequential selection: \"\n",
    "#     f\"{feature_names[sfs_forward.get_support()]}\"\n",
    "# )\n",
    "# print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n",
    "# print(\n",
    "#     \"Features selected by backward sequential selection: \"\n",
    "#     f\"{feature_names[sfs_backward.get_support()]}\"\n",
    "# )\n",
    "# print(f\"Done in {toc_bwd - tic_bwd:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X_std = scaler.fit_transform(x)\n",
    "\n",
    "# x_train_std, x_rest_std, y_train_std, y_rest_std = train_test_split(X_std, y, test_size=0.40, random_state = 2022)\n",
    "# x_val_std, x_test_std, y_val_std, y_test_std = train_test_split(X_std, y, test_size=0.50, random_state = 2022)\n",
    "\n",
    "# # Perform GridSearchCV to tune best-fit LR model\n",
    "# parameters = {\"C\": [10**-2,10**-1,10**0,10**1,10**2], \"penalty\": [\"l1\",\"l2\"]}\n",
    "\n",
    "# # lr_model = LogisticRegression(penalty=\"l1\", solver=\"liblinear\")\n",
    "# gs_model = GridSearchCV(estimator=model, param_grid=parameters)\n",
    "# gs_model.fit(x_train_std, y_train)\n",
    "# gs_model\n",
    "\n",
    "# # Train a LR model with best parameters\n",
    "# model = LogisticRegression(**gs_model.best_params_, solver=\"liblinear\")\n",
    "# model.fit(x_train_std, y_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00b91681c9a87862d55b4c1d596c6168a3d5b65766fc784fcff4be46a01cce2b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
